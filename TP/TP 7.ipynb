{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial.distance import jaccard\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a small collection of documents\n",
    "documents = [\n",
    "\n",
    "  # Cosine similarity\n",
    "  '''\n",
    "  The cosine similarity metric measures the angle between two vectors, making it highly effective for comparing documents.\n",
    "  By using TF-IDF representations, textual data can be transformed into feature vectors.\n",
    "  Tools like Gensim make it easy to compute cosine similarity for ranking document relevance\n",
    "  ''',\n",
    "  # Text Rank\n",
    "  ''' \n",
    "  The TextRank algorithm builds a graph of sentences to identify the most important ones based on their ranks. \n",
    "  It is often used for summarization and keyword extraction tasks in natural language processing. \n",
    "  By analyzing a graph, TextRank finds influential sentences or phrases for concise summaries.\n",
    "  ''',\n",
    "  # Cosine similarity\n",
    "  '''\n",
    "  When comparing documents, cosine similarity is one of the most commonly used metrics due to its efficiency. \n",
    "  Representing texts with TF-IDF vectors ensures that important terms are weighted properly in similarity calculations. \n",
    "  Libraries like Gensim streamline the use of cosine similarity for information retrieval tasks. \n",
    "  ''',\n",
    "  # Text Rank\n",
    "  ''' \n",
    "  In TextRank, sentences are treated as nodes connected by edges that represent their similarity. \n",
    "  This graph-based approach is widely used in keyword extraction and summarization. \n",
    "  By assigning importance to each node, TextRank ensures that the most significant content is retained.\n",
    "  ''',\n",
    "  # Cosine similarity\n",
    "  ''' \n",
    "  The cosine similarity metric provides an effective way to evaluate how similar two documents are based on their content.\n",
    "  Using TF-IDF vectors enhances this process by assigning higher weights to relevant terms.\n",
    "  The ease of implementing cosine similarity with tools like Gensim makes it a practical choice for many applications.\n",
    "  '''\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize, remove stop words, keep only alphabetic tokens\n",
    "    texts = [token.lemma_ for token in nlp(doc) if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(texts)\n",
    "\n",
    "# Apply preprocessing to documents\n",
    "documents = [preprocess_text(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cosine similarity metric measure angle vector make highly effective compare document TF IDF representation textual datum transform feature vector tool like Gensim easy compute cosine similarity rank document relevance',\n",
       " 'TextRank algorithm build graph sentence identify important one base rank summarization keyword extraction task natural language processing analyze graph TextRank find influential sentence phrase concise summary',\n",
       " 'compare document cosine similarity commonly metric efficiency represent text TF IDF vector ensure important term weight properly similarity calculation library like Gensim streamline use cosine similarity information retrieval task',\n",
       " 'TextRank sentence treat node connect edge represent similarity graph base approach widely keyword extraction summarization assign importance node TextRank ensure significant content retain',\n",
       " 'cosine similarity metric provide effective way evaluate similar document base content TF IDF vector enhance process assign high weight relevant term ease implement cosine similarity tool like Gensim make practical choice application']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4: Calculate Pairwise Similarities \n",
    "* Utiliser la cosine similarity pour mesurer la similitude entre les vecteurs. \n",
    "* Une valeur proche de 1 indique une forte similarité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gensims's Matrix Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Similarities - BoW (Gensim):\n",
      "[[1.         0.02906191 0.54054052 0.0632772  0.54799664]\n",
      " [0.02906191 1.         0.05812382 0.40824831 0.02946278]\n",
      " [0.54054052 0.05812382 1.         0.15819299 0.52059686]\n",
      " [0.0632772  0.40824831 0.15819299 1.         0.16037509]\n",
      " [0.54799664 0.02946278 0.5205968  0.16037509 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.lower().split() for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)  # Create a dictionary\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]  # Create BoW representation\n",
    "\n",
    "\n",
    "bow_index = MatrixSimilarity(bow_corpus)  # Build the similarity index\n",
    "bow_similarities = np.zeros((len(bow_corpus), len(bow_corpus)))\n",
    "\n",
    "for i, vec in enumerate(bow_corpus):\n",
    "    bow_similarities[i] = bow_index[vec]\n",
    "\n",
    "np.fill_diagonal(bow_similarities, 1.0)\n",
    "print(\"Pairwise Similarities - BoW (Gensim):\")\n",
    "print(bow_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Similarities - BoW:\n",
      "[[1.         0.02906191 0.54054054 0.0632772  0.54799662]\n",
      " [0.02906191 1.         0.05812382 0.40824829 0.02946278]\n",
      " [0.54054054 0.05812382 1.         0.158193   0.52059679]\n",
      " [0.0632772  0.40824829 0.158193   1.         0.16037507]\n",
      " [0.54799662 0.02946278 0.52059679 0.16037507 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Representations - Bag of Words (BoW), TF-IDF, and LDA\n",
    "# BoW Representation : Transforme chaque document en un vecteur qui représente la fréquence brute des mots.\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "# BoW Similarities\n",
    "bow_similarities = cosine_similarity(bow_matrix)\n",
    "\n",
    "# Display Results\n",
    "# Chaque cellule [i, j] représente la similarité cosine entre les documents i et j.\n",
    "# Diagone : Les valeurs sont 1, car chaque document est parfaitement similaire à lui-même.\n",
    "# Autres valeurs : Les similarités entre les documents dépendent du nombre de mots communs entre eux.\n",
    "print(\"Pairwise Similarities - BoW:\")\n",
    "print(bow_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BoW Jaccard Similarity:\n",
      " [[1.         0.0212766  0.24390244 0.02222222 0.27906977]\n",
      " [0.0212766  1.         0.04255319 0.18918919 0.01923077]\n",
      " [0.24390244 0.04255319 1.         0.06818182 0.24444444]\n",
      " [0.02222222 0.18918919 0.06818182 1.         0.08510638]\n",
      " [0.27906977 0.01923077 0.24444444 0.08510638 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Jaccard Similarity for BoW\n",
    "def calculate_jaccard(matrix):\n",
    "    binary_matrix = (matrix > 0).astype(int)\n",
    "    n_docs = binary_matrix.shape[0]\n",
    "    jaccard_sim = np.zeros((n_docs, n_docs))\n",
    "    for i in range(n_docs):\n",
    "        for j in range(n_docs):\n",
    "            jaccard_sim[i, j] = 1 - jaccard(binary_matrix[i], binary_matrix[j])\n",
    "    return jaccard_sim\n",
    "\n",
    "np.fill_diagonal(bow_similarities, 1.0)\n",
    "bow_jaccard_similarity = calculate_jaccard(bow_matrix.toarray())\n",
    "print(\"\\nBoW Jaccard Similarity:\\n\", bow_jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gensim's Matrix Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Similarities - TF-IDF (Gensim):\n",
      "[[1.         0.01962236 0.12517105 0.00250597 0.15272599]\n",
      " [0.01962236 1.         0.04047105 0.2084666  0.00568988]\n",
      " [0.12517105 0.04047105 1.         0.04745131 0.12497157]\n",
      " [0.00250597 0.2084666  0.04745131 1.         0.04788698]\n",
      " [0.15272601 0.00568988 0.12497157 0.04788698 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF using Gensim\n",
    "tfidf_model = models.TfidfModel(bow_corpus)  # Train TF-IDF model\n",
    "tfidf_corpus = [tfidf_model[doc] for doc in bow_corpus]  # Transform BoW corpus to TF-IDF\n",
    "\n",
    "# TF-IDF Similarities with Gensim\n",
    "tfidf_index = MatrixSimilarity(tfidf_corpus)  # Build the similarity index\n",
    "tfidf_similarities = np.zeros((len(tfidf_corpus), len(tfidf_corpus)))\n",
    "\n",
    "for i, vec in enumerate(tfidf_corpus):\n",
    "    tfidf_similarities[i] = tfidf_index[vec]\n",
    "\n",
    "print(\"\\nPairwise Similarities - TF-IDF (Gensim):\")\n",
    "print(tfidf_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Similarities - TF-IDF:\n",
      "[[1.         0.02696877 0.36870828 0.02863642 0.38242601]\n",
      " [0.02696877 1.         0.0548972  0.32919482 0.01817173]\n",
      " [0.36870828 0.0548972  1.         0.10349062 0.34996995]\n",
      " [0.02863642 0.32919482 0.10349062 1.         0.10521584]\n",
      " [0.38242601 0.01817173 0.34996995 0.10521584 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Representation : Ajoute une pondération basée sur l'importance des mots dans le corpus.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# TF-IDF Similarities\n",
    "tfidf_similarities = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Avec TF-IDF : Les similarités sont plus faibles car la pondération TF-IDF réduit l'importance des mots fréquents.\n",
    "# TF-IDF capture mieux les termes significatifs dans le contexte.\n",
    "\n",
    "print(\"\\nPairwise Similarities - TF-IDF:\")\n",
    "print(tfidf_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf Jaccard Similarity:\n",
      " [[1.         0.0212766  0.24390244 0.02222222 0.27906977]\n",
      " [0.0212766  1.         0.04255319 0.18918919 0.01923077]\n",
      " [0.24390244 0.04255319 1.         0.06818182 0.24444444]\n",
      " [0.02222222 0.18918919 0.06818182 1.         0.08510638]\n",
      " [0.27906977 0.01923077 0.24444444 0.08510638 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_jaccard(matrix):\n",
    "    binary_matrix = (matrix > 0).astype(int)\n",
    "    n_docs = binary_matrix.shape[0]\n",
    "    jaccard_sim = np.zeros((n_docs, n_docs))\n",
    "    for i in range(n_docs):\n",
    "        for j in range(n_docs):\n",
    "            jaccard_sim[i, j] = 1 - jaccard(binary_matrix[i], binary_matrix[j])\n",
    "    return jaccard_sim\n",
    "\n",
    "tfidf_jaccard_similarity = calculate_jaccard(tfidf_matrix.toarray())\n",
    "print(\"Tf-idf Jaccard Similarity:\\n\", tfidf_jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gensim's Matrix Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.057*\"similarity\" + 0.049*\"cosine\" + 0.034*\"vector\" + 0.034*\"document\" + 0.026*\"metric\" + 0.026*\"idf\" + 0.026*\"like\"\n",
      "Topic 1: 0.049*\"textrank\" + 0.038*\"sentence\" + 0.038*\"graph\" + 0.027*\"base\" + 0.027*\"summarization\" + 0.027*\"extraction\" + 0.027*\"keyword\"\n"
     ]
    }
   ],
   "source": [
    "# LDA Representation : Modélise chaque document comme une distribution sur des \"thèmes\" (topics).\n",
    "#                      Chaque thème est lui-même une distribution de mots.\n",
    "texts = [doc.lower().split() for doc in documents]  # Tokenize\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=50)\n",
    "lda_topics = [lda_model[doc] for doc in corpus]\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_topics=2, num_words=7):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Similarities (based on topic distributions) : Mesure la similarité entre les distributions des thèmes des documents.\n",
    "index = MatrixSimilarity(lda_model[corpus])\n",
    "lda_similarities = np.zeros((len(corpus), len(corpus)))\n",
    "\n",
    "for i, topic1 in enumerate(lda_model[corpus]):\n",
    "    lda_similarities[i] = index[topic1]\n",
    "\n",
    "np.fill_diagonal(bow_similarities, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Similarities - LDA:\n",
      "[[1.         0.03768451 0.9999994  0.04244778 0.99999964]\n",
      " [0.03768569 1.         0.03872294 0.99998868 0.03686775]\n",
      " [0.99999946 0.03872273 1.         0.0434858  0.99999827]\n",
      " [0.042446   0.99998862 0.04348305 1.         0.04162822]\n",
      " [0.99999964 0.03687339 0.99999821 0.04163681 0.99999994]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPairwise Similarities - LDA:\")\n",
    "print(lda_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA KL Divergence Similarity:\n",
      "[0.00 3.78 0.00 3.58 0.00]\n",
      "[3.87 0.00 3.81 0.00 3.91]\n",
      "[0.00 3.78 0.00 3.57 0.00]\n",
      "[3.83 0.00 3.78 0.00 3.88]\n",
      "[0.00 3.79 0.00 3.58 0.00]\n"
     ]
    }
   ],
   "source": [
    "# KL Divergence for LDA\n",
    "lda_distributions = np.array([\n",
    "    [dict(topic).get(i, 0) for i in range(lda_model.num_topics)] \n",
    "    for topic in lda_topics\n",
    "])\n",
    "\n",
    "def calculate_kl_divergence(distributions):\n",
    "    n_docs = distributions.shape[0]\n",
    "    kl_sim = np.zeros((n_docs, n_docs))\n",
    "    for i in range(n_docs):\n",
    "        for j in range(n_docs):\n",
    "            kl_sim[i, j] = entropy(distributions[i] + 1e-12, distributions[j] + 1e-12)\n",
    "    return kl_sim\n",
    "\n",
    "lda_kl_similarity = calculate_kl_divergence(lda_distributions)\n",
    "\n",
    "print(\"\\nLDA KL Divergence Similarity:\")\n",
    "for row in lda_kl_similarity:\n",
    "    formatted_row = [\"{:.2f}\".format(value) for value in row]  # Format to 2 decimal places\n",
    "    print(\"[{}]\".format(\" \".join(formatted_row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BoW (Bag of Words) :\n",
    "Les similarités ici sont relativement faibles, sauf entre certains documents (par exemple, [0,2] et [0,4]).\n",
    " Cela est attendu car BoW se concentre uniquement sur la fréquence brute des mots sans considérer leur importance ou \n",
    " le contexte. Les similarités élevées (ex. [0,2]) sont dues à des termes partagés fréquents. \n",
    " C'est simple mais souvent imprécis pour capturer des nuances sémantiques.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) :\n",
    "TF-IDF produit des similarités plus faibles que BoW, indiquant une pondération plus fine basée sur l'importance \n",
    "relative des mots (fréquence locale et rareté globale). Cela réduit l'effet des mots communs dans plusieurs documents\n",
    "(ex. \"the\", \"is\"). Cela améliore la précision pour des documents ayant des mots-clés significatifs, mais reste limité\n",
    "pour des concepts complexes.\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) :\n",
    "Les similarités avec LDA sont très élevées (proches de 1) entre presque tous les documents. \n",
    "Cela s'explique par la capacité de LDA à capturer les thématiques sous-jacentes plutôt que de se limiter aux mots exacts.\n",
    "Cependant, ces similarités excessivement élevées peuvent indiquer un sur-apprentissage ou un modèle mal ajusté.\n",
    "\n",
    "TF-IDF est généralement plus précis pour des données générales ou peu structurées car il gère les mots-clés \n",
    "avec plus de nuance.\n",
    "\n",
    "LDA est meilleur pour les relations sémantiques profondes (par exemple, \"chien\" et \"animal\"), \n",
    "mais il dépend fortement du réglage des paramètres (nombre de topics, etc.).\n",
    "\n",
    "BoW est simple et utile pour des tâches rudimentaires, mais souvent trop limité.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Bag of Words (BoW) :\n",
    "Avantages :\n",
    "    Simple à implémenter.\n",
    "    Peu coûteux en calculs.\n",
    "    Efficace pour des tâches où le vocabulaire est limité.\n",
    "Limites :\n",
    "    Ignore l'ordre des mots et le contexte.\n",
    "    Insensible aux synonymes ou relations sémantiques.\n",
    "    Sensible aux mots fréquents sans importance.\n",
    "TF-IDF :\n",
    "Avantages :\n",
    "    Pondère l'importance des mots en fonction de leur rareté dans le corpus.\n",
    "    Réduit l'impact des mots \"stopwords\".\n",
    "    Efficace pour capturer des mots significatifs.\n",
    "Limites :\n",
    "    Toujours basé sur des mots exacts (ne capture pas de sémantique profonde).\n",
    "    Moins performant pour de grands corpus complexes.\n",
    "LDA :\n",
    "Avantages :\n",
    "    Modélise les thèmes latents pour capturer des relations sémantiques entre mots.\n",
    "    Insensible aux mots exacts : regroupe des documents partageant des thématiques similaires.\n",
    "    Convient pour des corpus larges et variés.\n",
    "Limites :\n",
    "    Complexité computationnelle plus élevée.\n",
    "    Sensible au choix des hyperparamètres (ex. nombre de topics).\n",
    "    Peut produire des similarités biaisées si mal entraîné.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "LDA est un modèle probabiliste qui représente les documents comme une combinaison de thèmes (topics),\n",
    "chaque thème étant une distribution probabiliste sur les mots. Voici comment il capture des relations sémantiques :\n",
    "\n",
    "Thématiques communes :\n",
    "LDA regroupe des mots souvent utilisés ensemble dans différents documents (ex. \"chien\", \"chat\", \"animal\") \n",
    "sous un même thème, même s'ils n'apparaissent pas dans un même document.\n",
    "\n",
    "Probabilités partagées :\n",
    "Si deux documents sont composés majoritairement des mêmes thèmes, ils sont considérés comme similaires, \n",
    "même si leurs mots exacts diffèrent.\n",
    "\n",
    "Réduction de bruit :\n",
    "En modélisant des distributions latentes, LDA ignore les mots fréquents mais peu significatifs (stopwords).\n",
    "\n",
    "Contextualisation des mots :\n",
    "Les thèmes définis par LDA peuvent capturer des concepts abstraits (ex. \"finance\" pour \"banque\", \"argent\"), \n",
    "reliant ainsi des documents en fonction de leur sémantique plutôt que leur lexique exact.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
