{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_MAP = {\n",
    "    'N': 'NOUN',\n",
    "    'V': 'VERB',\n",
    "    'J':'ADJ',\n",
    "    'NNS' : 'NOUN'\n",
    "}\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", {'tags': ['N', 'V', 'J', 'NNS']}), \n",
    "    (\"Eat blue ham\", {'tags': ['V', 'J', 'N']}), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lang='en', output_dir=None, n_iter=3):\n",
    "\n",
    "    nlp = spacy.blank(lang) #Créer un modèle spacy vide pour l'anglais\n",
    "    tagger = nlp.add_pipe(\"tagger\") #Créer un modèle (tagger) qui attribue des tags aux mots d'un texte \n",
    "\n",
    "    for item in TAG_MAP:\n",
    "        tagger.add_label(item) #Ajouter des tags aux tagger\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "        print(losses)\n",
    "\n",
    "    test_text = \"I love cats\"\n",
    "    doc = nlp(test_text)\n",
    "    print('Tags', [(t.text, t.tag_, t.pos_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ner Taggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "('Who is Shaka Khan?', {\n",
    "    'entities': [(7, 17, 'PERSON')]\n",
    "}),\n",
    "('I like London and Berlin.', {\n",
    "    'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "}),\n",
    " ('I am learning Python', {\n",
    "    'entities': [(14, 20, 'TECH')]\n",
    "})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "        \n",
    "        if 'ner' not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe('ner', last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "        else:\n",
    "            ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    \n",
    "    # get names of other pipes to disable them during training \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        \n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    drop=0.5, # dropout-make it harder to memorise data\n",
    "                    sgd=optimizer, # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "            \n",
    "    # test the trained model\n",
    "    doc = nlp('I love Python')\n",
    "    print()\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
