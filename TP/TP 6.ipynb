{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [\n",
    "'alt.atheism',\n",
    "'talk.religion.misc',\n",
    "'comp.graphics',\n",
    "'sci.space',\n",
    "]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# warnings imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "#import dataset\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "shuffle=True, random_state=42)\n",
    "#save labels\n",
    "labels = dataset.target\n",
    "#get the unique labels\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "From: fitz@cse.ogi.edu (Bob Fitzsimmons)\n",
      "Subject: Re: VGA Graphics Library\n",
      "Keywords: C, library, graphics\n",
      "Article-I.D.: ogicse.53715\n",
      "Organization: Oregon Grad. Inst. Computer Science and Eng., Beaverton\n",
      "Lines: 26\n",
      "\n",
      "In article <2054@mwca.UUCP> bill@mwca.UUCP (Bill Sheppard) writes:\n",
      ">Many high-end graphics cards come with C source code for doing basic graphics\n",
      ">sorts of things (change colors, draw points/lines/polygons/fills, etc.).  Does\n",
      ">such a library exist for generic VGA graphics cards/chips, hopefully in the\n",
      ">public domain?  This would be for the purpose of compiling under a non-DOS\n",
      ">operating system running on a standard PC.\n",
      ">\n",
      "\n",
      "I'm also interested in info both public domain and commercial graphics library \n",
      "package to do PC VGA graphics.  \n",
      "\n",
      "I'm currently working on a realtime application running on a PCC with a \n",
      "non-DOS kernel that needs to do some simple graphics.  I'm not sure if \n",
      "reentrancy of the graphics library is going to be an issue or not.  \n",
      "I suspect I'll implement the display controller as a server process that \n",
      "handles graphics requests, queued on a mailbox, one at a time.  If this \n",
      "provides sufficiently frequent display updates then I believe that I can \n",
      "restrict all graphics operations to be performed by the server and thus \n",
      "constrain access to the library to a this single process and avoid the need\n",
      "for a reentrant graphics library.  \n",
      "\n",
      "Being fairly new to the realtime systems world I may be overlooking something,\n",
      "what do you think?\n",
      "\n",
      "Cheers,\n",
      "Bob Fitzsimmons\t\tfitz@cse.ogi.edu\t\t(503)297-3165\n",
      "\n",
      "\n",
      "Label (numerical): 1\n",
      "Label (category name): comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Display a text and its label\n",
    "index = 5  # Choose an index, e.g., 0 for the first document\n",
    "text = dataset.data[index]  # Retrieve the text\n",
    "label = dataset.target[index]  # Retrieve the corresponding label\n",
    "label_name = dataset.target_names[label]  # Get the human-readable label\n",
    "\n",
    "print(f\"Text:\\n{text}\\n\")\n",
    "print(f\"Label (numerical): {label}\")\n",
    "print(f\"Label (category name): {label_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* n_samples = le nombre de document\n",
    "* n_features = le nombre total de termes uniques dans le vocabulaire  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 3387, n_features: 24545\n"
     ]
    }
   ],
   "source": [
    "data = dataset.data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english',\n",
    "use_idf=True)\n",
    "X = tfidf_vectorizer.fit_transform(data)\n",
    "#The X object is now our input vector which contains the TF-IDF representation of our\n",
    "#dataset. \n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(max_iter=100, n_clusters=4, n_init=1)\n"
     ]
    }
   ],
   "source": [
    "#Dimensionality Reduction\n",
    "# Vectorizer results are normalized, which makes KMeans behave better\n",
    "    # Since LSA/SVD results are not normalized, we have to redo the normalization.\n",
    "\n",
    "    #If we do not normalize the data, variables with different scaling \n",
    "    # will be weighted differently in the distance formula \n",
    "    # that is being optimized during training.\n",
    "\n",
    "\n",
    "n_components = 5 #Sets the number of latent dimensions (topics) to which the data is reduced. \n",
    "                  #This controls how much the dimensionality of the dataset is reduced.\n",
    "# C'est le nombre de dimensions latentes (ou sujets, ou concepts cachés) que l'on souhaite conserver dans les données après la réduction.\n",
    "# Par exemple, si les données d'origine ont 100 dimensions (colonnes ou caractéristiques), on peut les réduire à seulement 5 dimensions grâce à n_components = 5.\n",
    "\n",
    "#Performs truncated singular value decomposition (SVD) on the input matrix \n",
    "#X to reduce its dimensionality.\n",
    "svd = TruncatedSVD(n_components)\n",
    "# C'est une méthode mathématique qui décompose une matrice en trois parties (singularités) pour en extraire l'information importante.\n",
    "# Le mot \"tronquée\" signifie que l'on conserve seulement les n_components dimensions les plus importantes et on ignore les autres.\n",
    "\n",
    "# Imaginez une matrice qui contient des informations sur 1000 documents et 10 000 mots (grande dimensionnalité).\n",
    "# Après réduction avec TruncatedSVD à 5 dimensions, chaque document sera représenté par seulement 5 chiffres (les concepts principaux), au lieu de 10 000 (les mots).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "#Combines the SVD and normalization steps into a single pipeline for streamlined processing. \n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "# C'est une fonction qui assemble plusieurs étapes de transformation en une seule \"pipeline\".\n",
    "# Cela permet d'appliquer plusieurs transformations aux données, étape par étape, de manière automatisée et ordonnée.\n",
    "\n",
    "\n",
    "\n",
    "#The final X is the input which we will be using. \n",
    "# It has been cleaned, TF-IDF transformed, and its dimensions reduced.\n",
    "X_reduced = lsa.fit_transform(X)\n",
    "\n",
    "#scikit-learn offers two implementations of kmeans:\n",
    "# either in mini-batches or without\n",
    "minibatch = False\n",
    "if minibatch:\n",
    "   km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "   init_size=1000, batch_size=1000)\n",
    "else:\n",
    "   km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "   # km = KMeans(n_clusters = 3, random_state = 0, n_init='auto')\n",
    "km.fit(X_reduced)\n",
    "# top words per cluster\n",
    "print(\"Clustering sparse data with %s\" % km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " graphics\n",
      " space\n",
      " image\n",
      " com\n",
      " nasa\n",
      " university\n",
      " posting\n",
      " images\n",
      " program\n",
      " file\n",
      "Cluster 1:\n",
      " god\n",
      " people\n",
      " com\n",
      " don\n",
      " say\n",
      " jesus\n",
      " think\n",
      " just\n",
      " believe\n",
      " morality\n",
      "Cluster 2:\n",
      " space\n",
      " henry\n",
      " toronto\n",
      " nasa\n",
      " access\n",
      " com\n",
      " digex\n",
      " pat\n",
      " gov\n",
      " zoo\n",
      "Cluster 3:\n",
      " sandvik\n",
      " kent\n",
      " apple\n",
      " newton\n",
      " com\n",
      " god\n",
      " jesus\n",
      " alink\n",
      " ksand\n",
      " cookamunga\n"
     ]
    }
   ],
   "source": [
    "# Ce code effectue une série d'opérations pour identifier les termes représentatifs (mots ou caractéristiques importantes) associés à des clusters formés par un algorithme de clustering\n",
    "\n",
    "# L'objectif est d'obtenir les termes principaux pour chaque cluster.\n",
    "\n",
    "\n",
    "\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "# svd : C'est l'objet de réduction de dimensionnalité basé sur la décomposition en valeurs singulières tronquées (Truncated SVD).\n",
    "# km : KMeans de Scikit-learn, qui a déjà identifié des centroïdes représentant les différents clusters.\n",
    "# Les centroïdes des clusters (km.cluster_centers_) sont dans l'espace réduit (celui produit par svd). Cette ligne applique une transformation inverse pour ramener les centroïdes à l'espace original (avant la réduction de dimensionnalité).\n",
    "# Pourquoi : Dans l'espace original, les dimensions correspondent aux termes réels (mots), ce qui permet de mieux interpréter les clusters.\n",
    "\n",
    "\n",
    "\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "# argsort() trie les indices des termes dans chaque cluster selon leurs valeurs dans les centroïdes.\n",
    "# [:, ::-1] signifie que l'ordre des indices est inversé pour avoir les termes avec les valeurs les plus grandes en premier.\n",
    "# Pourquoi : Les valeurs les plus grandes indiquent les termes les plus importants ou les plus représentatifs du cluster.\n",
    "\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "# tfidf_vectorizer : Un objet qui transforme le texte en une matrice TF-IDF, où chaque colonne correspond à un terme (mot).\n",
    "for i in range(true_k):\n",
    "   print(\"Cluster %d:\" % i)\n",
    "   for ind in order_centroids[i, :10]:\n",
    "      print(' %s' % terms[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First method:\n",
      "Homogeneity: 0.592\n",
      "Completeness: 0.669\n",
      "V-measure: 0.628\n",
      "Adjusted Rand-Index: 0.624\n",
      "Silhouette Coefficient: 0.006 \n"
     ]
    }
   ],
   "source": [
    "print(\"First method:\")\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000)) #Mesure la séparation entre les clusters en comparant les distances intra- et inter-clusters (valeurs proches de 1 indiquent de bons clusters).\n",
    "#Note: You might see different results, as machine learning \n",
    "# algorithms do not produce the exact same results each time.\n",
    "# km.predict(X_test) to test our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Méthode 2 K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second method:\n",
      "Cluster 0:\n",
      " space\n",
      " graphics\n",
      " nasa\n",
      " com\n",
      " access\n",
      " image\n",
      " university\n",
      " gov\n",
      " posting\n",
      " host\n",
      "Cluster 1:\n",
      " god\n",
      " people\n",
      " jesus\n",
      " don\n",
      " com\n",
      " say\n",
      " believe\n",
      " think\n",
      " bible\n",
      " just\n",
      "Cluster 2:\n",
      " sgi\n",
      " livesey\n",
      " keith\n",
      " wpd\n",
      " solntze\n",
      " jon\n",
      " com\n",
      " caltech\n",
      " morality\n",
      " moral\n",
      "Cluster 3:\n",
      " sandvik\n",
      " kent\n",
      " apple\n",
      " newton\n",
      " com\n",
      " jesus\n",
      " alink\n",
      " ksand\n",
      " god\n",
      " cookamunga\n"
     ]
    }
   ],
   "source": [
    "#imports the KMeans algorithm from the scikit-learn library and \n",
    "# creates an instance of it with three clusters, a random state of 0, \n",
    "# and automatic initialization\n",
    "#KMeans algorithm is a clustering algorithm that groups \n",
    "# similar data points together based on their distance from each other\n",
    "\n",
    "#random runs: This affects how the initial cluster centroids are chosen \n",
    "#and ensures consistent results across multiple runs.\n",
    "#n_init=auto: Automatically runs 10 initializations and picks the best one based on inertia (objective function).\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 0, n_init='auto')\n",
    "#The fit method is then called on the normalized training data \n",
    "# to train the KMeans model on the data.\n",
    "kmeans.fit(X)\n",
    "print(\"Second method:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "   print(\"Cluster %d:\" % i)\n",
    "   for ind in order_centroids[i, :10]:\n",
    "      print(' %s' % terms[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.390\n",
      "Completeness: 0.542\n",
      "V-measure: 0.453\n",
      "Adjusted Rand-Index: 0.391\n",
      "Silhouette Coefficient: 0.007 \n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Naïve Bayes Classification ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90       224\n",
      "           1       0.95      0.99      0.97       297\n",
      "           2       0.96      0.98      0.97       307\n",
      "           3       0.99      0.69      0.81       189\n",
      "\n",
      "    accuracy                           0.93      1017\n",
      "   macro avg       0.93      0.91      0.91      1017\n",
      "weighted avg       0.94      0.93      0.93      1017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- 2. Classification supervisée avec Naïve Bayes ----\n",
    "print(\"\\n=== Naïve Bayes Classification ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dataset.target, test_size=0.3, random_state=42)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison: Bag of Words vs TF-IDF ===\n",
      "Bag of Words Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       224\n",
      "           1       0.97      0.99      0.98       297\n",
      "           2       0.98      0.96      0.97       307\n",
      "           3       0.94      0.85      0.89       189\n",
      "\n",
      "    accuracy                           0.95      1017\n",
      "   macro avg       0.94      0.94      0.94      1017\n",
      "weighted avg       0.95      0.95      0.95      1017\n",
      "\n",
      "TF-IDF Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90       224\n",
      "           1       0.95      0.99      0.97       297\n",
      "           2       0.96      0.98      0.97       307\n",
      "           3       0.99      0.69      0.81       189\n",
      "\n",
      "    accuracy                           0.93      1017\n",
      "   macro avg       0.93      0.91      0.91      1017\n",
      "weighted avg       0.94      0.93      0.93      1017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- 3. Comparaison Bag of Words vs TF-IDF ----\n",
    "print(\"\\n=== Comparison: Bag of Words vs TF-IDF ===\")\n",
    "\n",
    "# Vectorisation Bag of Words\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', max_df=0.5, min_df=2)\n",
    "X_bow = bow_vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "# Naïve Bayes avec Bag of Words\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, dataset.target, test_size=0.3, random_state=42)\n",
    "nb_bow = MultinomialNB()\n",
    "nb_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "# Prédictions et évaluation Bag of Words\n",
    "y_pred_bow = nb_bow.predict(X_test_bow)\n",
    "print(\"Bag of Words Performance:\")\n",
    "print(classification_report(y_test_bow, y_pred_bow))\n",
    "\n",
    "# Naïve Bayes avec TF-IDF (déjà fait, rappel des résultats pour comparaison)\n",
    "print(\"TF-IDF Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Best Model on New Data ===\n",
      "Document: The launch of the satellite was a major milestone in space exploration.\n",
      "Predicted Category: sci.space\n",
      "\n",
      "Document: Scientists developed a new vaccine that promises to combat emerging viruses.\n",
      "Predicted Category: sci.space\n",
      "\n",
      "Document: The new computer graphics card delivers stunning performance.\n",
      "Predicted Category: comp.graphics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- 4. Tester le meilleur modèle sur de nouvelles données ----\n",
    "print(\"\\n=== Testing Best Model on New Data ===\")\n",
    "\n",
    "new_documents = [\n",
    "    \"The launch of the satellite was a major milestone in space exploration.\",\n",
    "    \"Scientists developed a new vaccine that promises to combat emerging viruses.\",\n",
    "    \"The new computer graphics card delivers stunning performance.\"\n",
    "]\n",
    "\n",
    "tfidf_vectorizer.fit(dataset.data)\n",
    "# Utiliser Naïve Bayes (modèle supervisé) sur les nouveaux textes\n",
    "new_X_tfidf = tfidf_vectorizer.transform(new_documents)\n",
    "new_predictions = nb.predict(new_X_tfidf)\n",
    "\n",
    "# Afficher les résultats\n",
    "for doc, pred in zip(new_documents, new_predictions):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Predicted Category: {dataset.target_names[pred]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
