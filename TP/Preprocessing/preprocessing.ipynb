{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_summary_\\n      \\n    Ce que contient un objet doc :\\n        Un objet doc n’est pas juste une liste de mots ; c’est un conteneur complexe qui comprend de nombreuses informations sur chaque mot. Voici quelques éléments que doc peut contenir :\\n\\n        Tokens : Chaque mot du texte est transformé en un token que spaCy peut manipuler. Par exemple, si le texte est \"I love programming\", l\\'objet doc contiendra trois tokens : \"I\", \"love\" et \"programming\".\\n        Part-of-Speech (POS) Tags : Chaque token est étiqueté avec sa catégorie grammaticale, comme verbe, nom, adjectif, etc.\\n        Entités nommées (NER) : spaCy identifie des entités comme des personnes, des lieux, des dates, des organisations, etc.\\n        Dépendances syntaxiques : spaCy analyse la structure grammaticale du texte et montre comment les mots sont reliés entre eux (par exemple, quel mot est le sujet, quel mot est le verbe, etc.).\\n        Lemmes : Chaque token a aussi une forme de base (lemme). Par exemple, \"running\" devient \"run\".\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"_summary_\n",
    "          \n",
    "        Ce que contient un objet doc :\n",
    "            Un objet doc n’est pas juste une liste de mots ; c’est un conteneur complexe qui comprend de nombreuses informations sur chaque mot. Voici quelques éléments que doc peut contenir :\n",
    "\n",
    "            Tokens : Chaque mot du texte est transformé en un token que spaCy peut manipuler. Par exemple, si le texte est \"I love programming\", l'objet doc contiendra trois tokens : \"I\", \"love\" et \"programming\".\n",
    "            Part-of-Speech (POS) Tags : Chaque token est étiqueté avec sa catégorie grammaticale, comme verbe, nom, adjectif, etc.\n",
    "            Entités nommées (NER) : spaCy identifie des entités comme des personnes, des lieux, des dates, des organisations, etc.\n",
    "            Dépendances syntaxiques : spaCy analyse la structure grammaticale du texte et montre comment les mots sont reliés entre eux (par exemple, quel mot est le sujet, quel mot est le verbe, etc.).\n",
    "            Lemmes : Chaque token a aussi une forme de base (lemme). Par exemple, \"running\" devient \"run\".\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Cette partie convertit tout le texte en minuscules pour uniformiser la casse et éviter les différences entre, par exemple, \"Apple\" et \"apple\".\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Initialize an empty list to hold the preprocessed tokens\n",
    "    preprocessed_tokens = []\n",
    "\n",
    "    # Cette ligne utilise la liste des stopwords (mots très communs comme \"and\", \"the\", \"in\", etc.) de spaCy en anglais. Ces mots seront filtrés car ils n’apportent généralement pas d’information utile.\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    # calculate the token frequency in the text\n",
    "    token_counts = Counter([token.text for token in doc if token.is_alpha])\n",
    "\n",
    "    # Preprocessing\n",
    "    for token in doc: #for each token check \n",
    "        if (token.is_alpha and # to remove non-alphabetic caracters (punctuations),\n",
    "            token.text not in stopwords and # to remove stopwords,\n",
    "            token_counts[token.text] > 0 and # to remove rare words,\n",
    "            token_counts[token.text] < 10):  #  Élimine les mots trop fréquents (qui apparaissent plus de 10 fois), car ces mots sont souvent peu informatifs et peuvent être des répétitions ou des termes courants.\n",
    "            \n",
    "            # réduit le mot à sa forme de base\n",
    "            preprocessed_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(preprocessed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge nltk\n",
    "\n",
    "import nltk\n",
    "try :\n",
    "    from nltk.corpus import reuters #import reuters' documents from the nltk library\n",
    "except Exception as e :\n",
    "    print(f'une erreur se produit {e}')\n",
    "\n",
    "fileids = reuters.fileids() #Get the documents ids\n",
    "\n",
    "\n",
    "text = reuters.raw(fileids[0]) # Get the text of the first document\n",
    "print(text[:830]) # Take only the first 830 caractére\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preprocess_text(text) #Apply the preprocessing\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
